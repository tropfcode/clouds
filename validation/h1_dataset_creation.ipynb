{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of H1 Dataset\n",
    "\n",
    "The new HGG ISCCP dataset spans the years 1983-2009 and consists of a netCDF file for every 3 hour period. \n",
    "Trying to analyze PC-TAU information by accessing the raw data directly is inefficient -- therefore, the relevant\n",
    "data from each file will be extracted and aggregated by year.\n",
    "\n",
    "For now, the analyzed data will be stored as a numpy array (file ending as `.npy`) that can be accessed via the `np.load()` function.\n",
    "\n",
    "Each `.npy` file will be a 3D numpy array with shape (12, 12, 64800) which corresponds to (time (month), weather state, gridbox)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Creation Process\n",
    "\n",
    "The raw data is arranged in a HardDrive in directories of year, subdirectories of month, with raw netCDF files for every 3 hour period of that month.\n",
    "The procedure to create an H1 dataset for a given year is as follows:\n",
    "\n",
    "1. Extract the relevant variables from a netCDF file representing 3 hours of data\n",
    "2. Normalize the gridbox data (pctaudist variable) using the n_total variable to convert from pixel count to percentage\n",
    "3. Compute the Euclidean distance of each gridbox with every PC-TAU histogram from the accepted Weather States\n",
    "4. Find the smallest distance for each gridbox to determine the corresponding Weather State of the gridbox\n",
    "5. Increment the counter of the numpy matrix cell keeping track of the specific time, Weather State, and gridbox\n",
    "\n",
    "For example, if the first gridbox of a given 3 hour period for the month of Janurary corresponds to Weather State 7, then the following happends: `data[0,7,gridbox] += 1`\n",
    "\n",
    "### Note: Reference analysis.f90 for the functions used to normalize gridbox data and for computing gridbox weather states\n",
    "\n",
    "A demonstration of creating a single year of analyzed data is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import netCDF4\n",
    "import analysispy\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for  2010 506.1533224582672\n",
      "Time for  2011 512.2897651195526\n",
      "Time for  2012 522.095691204071\n"
     ]
    }
   ],
   "source": [
    "base_path = '/run/media/dtropf/Seagate Backup Plus Drive/hgg_data/'\n",
    "save_path = '/home/dtropf/Documents/NASA/clouds/validation/test_data/'\n",
    "\n",
    "# Load PC-TAU histograms of accepted Weather States\n",
    "pctau_list = np.loadtxt(os.getcwd()+'/'+'pctau.txt').reshape(12, 7, 6).transpose((0,2,1))/100.0\n",
    "\n",
    "# Years to make datasets (excludes final year via python syntax)\n",
    "years = [str(i) for i in range(2010, 2013)]\n",
    "\n",
    "files = []\n",
    "\n",
    "for year in years:\n",
    "    year_path = base_path+year\n",
    "    all_data = np.zeros((12, 12, 360*180))\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # Loop into each month directory for the given year\n",
    "    for i, f1 in enumerate(sorted(os.listdir(year_path))):\n",
    "        #if 'ISCCP' in f1:\n",
    "        month_path = year_path+'/'+f1\n",
    "\n",
    "        # Loop over every 3 hour netCDF file\n",
    "        for f2 in (sorted(os.listdir(month_path))):\n",
    "            data_path = month_path+'/'+f2\n",
    "            with netCDF4.Dataset(data_path) as data:\n",
    "                pctaudist = data['n_pctaudist'][:,:,:]\n",
    "                sqlonbeg = data['sqlon_beg'][:]\n",
    "                sqlonend = data['sqlon_end'][:]\n",
    "                eqlat = data['eqlat_index'][:]\n",
    "                n_total = data['n_total'][:]\n",
    "\n",
    "                # Normalize pctaudist from pixel count to \n",
    "                # percentage using fortran function\n",
    "                pctaudist_norm = analysispy.fnorm(pctaudist, n_total, pctaudist.mask)\n",
    "\n",
    "                # Calculate weatherstate for each of the 64800 gridboxes\n",
    "                ws_num = analysispy.findws(pctaudist_norm, pctau_list, pctaudist.mask, sqlonbeg, sqlonend, eqlat)\n",
    "\n",
    "                # Reshape data to fit the shape of (12, 12, 64800)\n",
    "                for j in range(12):\n",
    "                    all_data[i,j,:] = all_data[i,j,:] + ws_num[:,:,j].reshape(360*180)\n",
    "\n",
    "                # Memory cleanup\n",
    "                del data\n",
    "                del pctaudist\n",
    "                del sqlonbeg\n",
    "                del eqlat\n",
    "                del n_total\n",
    "                gc.collect()\n",
    "    t2 = time.time()\n",
    "            \n",
    "    print('Time for ',year,t2-t1)\n",
    "    \n",
    "    # Save 3D numpy array\n",
    "    np.save(save_path+'H1_'+year, all_data)\n",
    "    \n",
    "    # Memory cleanup\n",
    "    del all_data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using H1 Dataset\n",
    "\n",
    "Analyzed data is stored on the level of month of a given year. This means that the max temporal resolution of the data is month.\n",
    "\n",
    "For example, should a map of Weather State 6 of July of 1999 be desired, then one can acquire that specific data like so:\n",
    "\n",
    ">> `data_path = 'H1_1999.npy'`\n",
    "\n",
    ">>`data = np.load(data_path)`\n",
    "\n",
    ">>`desired_data = data[7,6,:].reshape(360, 180)`\n",
    "\n",
    "Note that all data along the 3rd axis is arranged in Equal-Angle format and can be reshaped in the format of LON,LAT at will (another rotation of data is necessary for LAT, LON arrangment).\n",
    "\n",
    "See the following for examples of H1 Dataset use:\n",
    "\n",
    "1. h1_wsmaps.ipynb for vizualizing the dataset\n",
    "2. api_test.ipynb for easily accessing the data using custom functions for such\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
